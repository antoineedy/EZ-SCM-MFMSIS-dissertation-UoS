[i 0801 14:18:33.298216 52 lock.py:85] Create lock file:/user/HS400/ae01116/.cache/jittor/jt1.3.8/g++7.5.0/py3.9.19/Linux-4.18.0-4x44/AMDEPYC753232-x73/jittor.lock
[i 0801 14:18:33.349574 52 compiler.py:956] Jittor(1.3.8.5) src: /mnt/fast/nobackup/scratch4weeks/ae01116/zegenv/lib/python3.9/site-packages/jittor
[i 0801 14:18:33.354124 52 compiler.py:957] g++ at /usr/bin/g++(7.5.0)
[i 0801 14:18:33.354242 52 compiler.py:958] cache_path: /user/HS400/ae01116/.cache/jittor/jt1.3.8/g++7.5.0/py3.9.19/Linux-4.18.0-4x44/AMDEPYC753232-x73/default
[i 0801 14:18:33.359444 52 __init__.py:411] Found nvcc(11.0.221) at /usr/local/cuda/bin/nvcc.
[i 0801 14:18:33.364810 52 __init__.py:411] Found addr2line(2.30) at /usr/bin/addr2line.
[i 0801 14:18:33.499532 52 compiler.py:1011] cuda key:cu11.0.221_sm_86
[i 0801 14:18:33.545580 52 compiler.py:34] Create cache dir: /user/HS400/ae01116/.cache/jittor/jt1.3.8/g++7.5.0/py3.9.19/Linux-4.18.0-4x44/AMDEPYC753232-x73/default/cu11.0.221_sm_86
[i 0801 14:18:33.548300 52 compiler.py:34] Create cache dir: /user/HS400/ae01116/.cache/jittor/jt1.3.8/g++7.5.0/py3.9.19/Linux-4.18.0-4x44/AMDEPYC753232-x73/default/cu11.0.221_sm_86/jit
[i 0801 14:18:33.549908 52 compiler.py:34] Create cache dir: /user/HS400/ae01116/.cache/jittor/jt1.3.8/g++7.5.0/py3.9.19/Linux-4.18.0-4x44/AMDEPYC753232-x73/default/cu11.0.221_sm_86/obj_files
[i 0801 14:18:33.551447 52 compiler.py:34] Create cache dir: /user/HS400/ae01116/.cache/jittor/jt1.3.8/g++7.5.0/py3.9.19/Linux-4.18.0-4x44/AMDEPYC753232-x73/default/cu11.0.221_sm_86/gen
[i 0801 14:18:33.553154 52 compiler.py:34] Create cache dir: /user/HS400/ae01116/.cache/jittor/jt1.3.8/g++7.5.0/py3.9.19/Linux-4.18.0-4x44/AMDEPYC753232-x73/default/cu11.0.221_sm_86/tmp
[i 0801 14:18:33.554897 52 compiler.py:34] Create cache dir: /user/HS400/ae01116/.cache/jittor/jt1.3.8/g++7.5.0/py3.9.19/Linux-4.18.0-4x44/AMDEPYC753232-x73/default/cu11.0.221_sm_86/checkpoints
Compiling jittor_core(25/151) used: 2.011s eta: 10.134sCompiling jittor_core(26/151) used: 2.048s eta: 9.844sCompiling jittor_core(27/151) used: 2.193s eta: 10.071sCompiling jittor_core(28/151) used: 2.200s eta: 9.663sCompiling jittor_core(29/151) used: 2.217s eta: 9.326sCompiling jittor_core(30/151) used: 2.493s eta: 10.054sCompiling jittor_core(31/151) used: 2.500s eta: 9.679sCompiling jittor_core(32/151) used: 2.559s eta: 9.517sCompiling jittor_core(33/151) used: 2.583s eta: 9.238sCompiling jittor_core(34/151) used: 2.735s eta: 9.410sCompiling jittor_core(35/151) used: 2.757s eta: 9.136sCompiling jittor_core(36/151) used: 2.872s eta: 9.173sCompiling jittor_core(37/151) used: 2.983s eta: 9.192sCompiling jittor_core(38/151) used: 3.053s eta: 9.079sCompiling jittor_core(39/151) used: 3.080s eta: 8.845sCompiling jittor_core(40/151) used: 3.150s eta: 8.742sCompiling jittor_core(41/151) used: 3.187s eta: 8.551sCompiling jittor_core(42/151) used: 3.213s eta: 8.339sCompiling jittor_core(43/151) used: 3.388s eta: 8.510sCompiling jittor_core(44/151) used: 3.411s eta: 8.295sCompiling jittor_core(45/151) used: 3.457s eta: 8.143sCompiling jittor_core(46/151) used: 3.649s eta: 8.329sCompiling jittor_core(47/151) used: 3.651s eta: 8.080sCompiling jittor_core(48/151) used: 3.708s eta: 7.957sCompiling jittor_core(49/151) used: 3.839s eta: 7.992sCompiling jittor_core(50/151) used: 3.970s eta: 8.020sCompiling jittor_core(51/151) used: 4.009s eta: 7.862sCompiling jittor_core(52/151) used: 4.033s eta: 7.679sCompiling jittor_core(53/151) used: 4.042s eta: 7.474sCompiling jittor_core(54/151) used: 4.095s eta: 7.355sCompiling jittor_core(55/151) used: 4.135s eta: 7.217sCompiling jittor_core(56/151) used: 4.317s eta: 7.324sCompiling jittor_core(57/151) used: 4.319s eta: 7.122sCompiling jittor_core(58/151) used: 4.412s eta: 7.074sCompiling jittor_core(59/151) used: 4.521s eta: 7.050sCompiling jittor_core(60/151) used: 4.539s eta: 6.883sCompiling jittor_core(61/151) used: 4.597s eta: 6.782sCompiling jittor_core(62/151) used: 4.599s eta: 6.602sCompiling jittor_core(63/151) used: 4.739s eta: 6.619sCompiling jittor_core(64/151) used: 4.893s eta: 6.651sCompiling jittor_core(65/151) used: 4.893s eta: 6.474sCompiling jittor_core(66/151) used: 4.981s eta: 6.415sCompiling jittor_core(67/151) used: 4.991s eta: 6.257sCompiling jittor_core(68/151) used: 5.009s eta: 6.113sCompiling jittor_core(69/151) used: 5.038s eta: 5.987sCompiling jittor_core(70/151) used: 5.097s eta: 5.898sCompiling jittor_core(71/151) used: 5.149s eta: 5.801sCompiling jittor_core(72/151) used: 5.310s eta: 5.826sCompiling jittor_core(73/151) used: 5.339s eta: 5.705sCompiling jittor_core(74/151) used: 5.341s eta: 5.558sCompiling jittor_core(75/151) used: 5.344s eta: 5.416sCompiling jittor_core(76/151) used: 5.394s eta: 5.323sCompiling jittor_core(77/151) used: 5.425s eta: 5.214sCompiling jittor_core(78/151) used: 5.484s eta: 5.132sCompiling jittor_core(79/151) used: 5.498s eta: 5.011sCompiling jittor_core(80/151) used: 5.568s eta: 4.942sCompiling jittor_core(81/151) used: 5.592s eta: 4.832sCompiling jittor_core(82/151) used: 5.644s eta: 4.749sCompiling jittor_core(83/151) used: 5.691s eta: 4.662sCompiling jittor_core(84/151) used: 5.943s eta: 4.740sCompiling jittor_core(85/151) used: 6.088s eta: 4.727sCompiling jittor_core(86/151) used: 6.090s eta: 4.603sCompiling jittor_core(87/151) used: 6.093s eta: 4.483sCompiling jittor_core(88/151) used: 6.150s eta: 4.403sCompiling jittor_core(89/151) used: 6.221s eta: 4.334sCompiling jittor_core(90/151) used: 6.341s eta: 4.298sCompiling jittor_core(91/151) used: 6.367s eta: 4.198sCompiling jittor_core(92/151) used: 6.482s eta: 4.157sCompiling jittor_core(93/151) used: 6.530s eta: 4.073sCompiling jittor_core(94/151) used: 6.548s eta: 3.971sCompiling jittor_core(95/151) used: 6.557s eta: 3.865sCompiling jittor_core(96/151) used: 6.583s eta: 3.772sCompiling jittor_core(97/151) used: 6.683s eta: 3.720sCompiling jittor_core(98/151) used: 6.692s eta: 3.619sCompiling jittor_core(99/151) used: 6.772s eta: 3.557sCompiling jittor_core(100/151) used: 6.802s eta: 3.469sCompiling jittor_core(101/151) used: 6.851s eta: 3.391sCompiling jittor_core(102/151) used: 6.906s eta: 3.318sCompiling jittor_core(103/151) used: 7.194s eta: 3.353sCompiling jittor_core(104/151) used: 7.203s eta: 3.255sCompiling jittor_core(105/151) used: 7.305s eta: 3.200sCompiling jittor_core(106/151) used: 7.306s eta: 3.102sCompiling jittor_core(107/151) used: 7.376s eta: 3.033sCompiling jittor_core(108/151) used: 7.409s eta: 2.950sCompiling jittor_core(109/151) used: 7.525s eta: 2.900sCompiling jittor_core(110/151) used: 7.595s eta: 2.831sCompiling jittor_core(111/151) used: 7.635s eta: 2.751sCompiling jittor_core(112/151) used: 7.666s eta: 2.669sCompiling jittor_core(113/151) used: 7.766s eta: 2.611sCompiling jittor_core(114/151) used: 7.772s eta: 2.522sCompiling jittor_core(115/151) used: 7.853s eta: 2.458sCompiling jittor_core(116/151) used: 7.925s eta: 2.391sCompiling jittor_core(117/151) used: 7.949s eta: 2.310sCompiling jittor_core(118/151) used: 8.083s eta: 2.261sCompiling jittor_core(119/151) used: 8.140s eta: 2.189sCompiling jittor_core(120/151) used: 8.246s eta: 2.130sCompiling jittor_core(121/151) used: 8.360s eta: 2.073sCompiling jittor_core(122/151) used: 8.433s eta: 2.004sCompiling jittor_core(123/151) used: 8.436s eta: 1.920sCompiling jittor_core(124/151) used: 8.505s eta: 1.852sCompiling jittor_core(125/151) used: 8.526s eta: 1.774sCompiling jittor_core(126/151) used: 8.528s eta: 1.692sCompiling jittor_core(127/151) used: 8.613s eta: 1.628sCompiling jittor_core(128/151) used: 8.779s eta: 1.577sCompiling jittor_core(129/151) used: 8.851s eta: 1.509sCompiling jittor_core(130/151) used: 8.870s eta: 1.433sCompiling jittor_core(131/151) used: 8.908s eta: 1.360sCompiling jittor_core(132/151) used: 8.923s eta: 1.284sCompiling jittor_core(133/151) used: 8.934s eta: 1.209sCompiling jittor_core(134/151) used: 8.956s eta: 1.136sCompiling jittor_core(135/151) used: 8.997s eta: 1.066sCompiling jittor_core(136/151) used: 9.140s eta: 1.008sCompiling jittor_core(137/151) used: 9.179s eta: 0.938sCompiling jittor_core(138/151) used: 9.180s eta: 0.865sCompiling jittor_core(139/151) used: 9.212s eta: 0.795sCompiling jittor_core(140/151) used: 9.258s eta: 0.727sCompiling jittor_core(141/151) used: 9.346s eta: 0.663sCompiling jittor_core(142/151) used: 9.385s eta: 0.595sCompiling jittor_core(143/151) used: 9.454s eta: 0.529sCompiling jittor_core(144/151) used: 9.517s eta: 0.463sCompiling jittor_core(145/151) used: 9.574s eta: 0.396sCompiling jittor_core(146/151) used: 9.644s eta: 0.330sCompiling jittor_core(147/151) used: 9.703s eta: 0.264sCompiling jittor_core(148/151) used: 10.518s eta: 0.213sCompiling jittor_core(149/151) used: 10.927s eta: 0.147sCompiling jittor_core(150/151) used: 11.285s eta: 0.075sCompiling jittor_core(151/151) used: 17.524s eta: 0.000s
Compiling libcutt(8/9) used: 3.782s eta: 0.473sCompiling libcutt(9/9) used: 7.955s eta: 0.000s
Compiling gen_ops_mkl_conv_mkl_conv_backward_w_mkl_test_mkl____hash4cc03b(7/7) used: 2.278s eta: 0.000s
Compiling gen_ops_cudnn_rnn_cudnn_conv_backward_w_cudnn_conv___hash8b3aa2(16/16) used: 3.470s eta: 0.000s
Torch cache cleared
Loading config from:  /mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/configs/voc12/clip_rc_zero_vit-b_512x512_40k_voc_10_16.py
Resize the pos_embed shape from (197, 768) to [1025,768,]
Making visible mask for zero-shot setting: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  -1  -1  -1
  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1
  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1
  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1
  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1
  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1
  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1
  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1
  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1
  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1
  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1
  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1
  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1
  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1
  -1  -1  -1 255]
--------------------------------------
Finetune layer in segmentor: backbone.prompt_embeddings
Finetune layer in segmentor: backbone.deep_prompt_embeddings
Finetune layer in segmentor: backbone.prompt_proj.weight
Finetune layer in segmentor: backbone.prompt_proj.bias
Finetune layer in segmentor: backbone.prompt_norm.weight
Finetune layer in segmentor: backbone.prompt_norm.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.0.linear1.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.0.linear2.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.0.norm2.weight
Finetune layer in segmentor: decode_head.decoder_q.layers.0.norm2.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.0.norm3.weight
Finetune layer in segmentor: decode_head.decoder_q.layers.0.norm3.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.0.multihead_attn.q.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.0.multihead_attn.k.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.0.multihead_attn.v.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.0.multihead_attn.proj.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.1.linear1.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.1.linear2.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.1.norm2.weight
Finetune layer in segmentor: decode_head.decoder_q.layers.1.norm2.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.1.norm3.weight
Finetune layer in segmentor: decode_head.decoder_q.layers.1.norm3.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.1.multihead_attn.q.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.1.multihead_attn.k.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.1.multihead_attn.v.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.1.multihead_attn.proj.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.2.linear1.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.2.linear2.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.2.norm2.weight
Finetune layer in segmentor: decode_head.decoder_q.layers.2.norm2.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.2.norm3.weight
Finetune layer in segmentor: decode_head.decoder_q.layers.2.norm3.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.2.multihead_attn.q.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.2.multihead_attn.k.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.2.multihead_attn.v.bias
Finetune layer in segmentor: decode_head.decoder_q.layers.2.multihead_attn.proj.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.0.linear1.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.0.linear2.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.0.norm2.weight
Finetune layer in segmentor: decode_head.decoder_v.layers.0.norm2.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.0.norm3.weight
Finetune layer in segmentor: decode_head.decoder_v.layers.0.norm3.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.0.multihead_attn.q.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.0.multihead_attn.k.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.0.multihead_attn.v.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.0.multihead_attn.proj.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.1.linear1.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.1.linear2.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.1.norm2.weight
Finetune layer in segmentor: decode_head.decoder_v.layers.1.norm2.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.1.norm3.weight
Finetune layer in segmentor: decode_head.decoder_v.layers.1.norm3.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.1.multihead_attn.q.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.1.multihead_attn.k.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.1.multihead_attn.v.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.1.multihead_attn.proj.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.2.linear1.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.2.linear2.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.2.norm2.weight
Finetune layer in segmentor: decode_head.decoder_v.layers.2.norm2.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.2.norm3.weight
Finetune layer in segmentor: decode_head.decoder_v.layers.2.norm3.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.2.multihead_attn.q.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.2.multihead_attn.k.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.2.multihead_attn.v.bias
Finetune layer in segmentor: decode_head.decoder_v.layers.2.multihead_attn.proj.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_q.0.linear1.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_q.0.linear2.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_q.0.norm2.weight
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_q.0.norm2.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_q.0.norm3.weight
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_q.0.norm3.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_q.0.multihead_attn.q.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_q.0.multihead_attn.k.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_q.0.multihead_attn.v.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_q.0.multihead_attn.proj.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_v.0.linear1.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_v.0.linear2.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_v.0.norm2.weight
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_v.0.norm2.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_v.0.norm3.weight
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_v.0.norm3.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_v.0.multihead_attn.q.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_v.0.multihead_attn.k.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_v.0.multihead_attn.v.bias
Finetune layer in segmentor: decode_head.recovery_decoder.decouple_v.0.multihead_attn.proj.bias
Finetune layer in segmentor: decode_head.recovery_decoder.linear_q_in.bias
Finetune layer in segmentor: decode_head.recovery_decoder.linear_k_in.bias
Finetune layer in segmentor: decode_head.recovery_decoder.linear_q_out.bias
Finetune layer in segmentor: decode_head.recovery_decoder.linear_k_out.bias
Finetune layer in segmentor: decode_head.lateral_proj.bias
Finetune layer in segmentor: decode_head.q_proj.bias
CLASSES: &id003 !!python/tuple
- aeroplane
- bicycle
- bird
- boat
- bottle
- bus
- car
- cat
- chair
- cow
- diningtable
- dog
- horse
- motorbike
- person
- pottedplant
- sheep
- sofa
- train
- tvmonitor
base: /mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos
base_class:
- 0
- 1
- 2
- 3
- 4
- 5
- 6
- 7
- 8
- 9
- 10
- 11
- 12
- 13
- 14
base_scratch: /mnt/fast/nobackup/scratch4weeks/ae01116
both_class:
- 0
- 1
- 2
- 3
- 4
- 5
- 6
- 7
- 8
- 9
- 10
- 11
- 12
- 13
- 14
- 15
- 16
- 17
- 18
- 19
checkpoint_interval: 2000
crop_size: &id001 !!python/tuple
- 512
- 512
data_root: /mnt/fast/nobackup/scratch4weeks/ae01116/data/VOC2012
dataset:
  train:
    ann_dir: SegmentationClassAug
    batch_size: 4
    data_root: /mnt/fast/nobackup/scratch4weeks/ae01116/data/VOC2012
    drop_last: false
    img_dir: JPEGImages
    num_workers: 8
    pipeline:
    - type: LoadImageFromFile
    - reduce_zero_label: true
      type: LoadAnnotations
    - img_scale: &id002 !!python/tuple
      - 2048
      - 512
      ratio_range: &id004 !!python/tuple
      - 0.5
      - 2.0
      type: Resize
    - cat_max_ratio: 0.75
      crop_size: *id001
      type: RandomCrop
    - prob: 0.5
      type: RandomFlip
    - type: PhotoMetricDistortion
    - mean:
      - 123.675
      - 116.28
      - 103.53
      std:
      - 58.395
      - 57.12
      - 57.375
      to_rgb: true
      type: Normalize
    - pad_val: 0
      seg_pad_val: 255
      size: *id001
      type: Pad
    - type: DefaultFormatBundle
    - keys:
      - img
      - gt_semantic_seg
      type: Collect
    shuffle: true
    split: ImageSets/Segmentation/trainaug.txt
    type: ZeroPascalVOCDataset20
  val:
    ann_dir: SegmentationClass
    batch_size: 1
    data_root: /mnt/fast/nobackup/scratch4weeks/ae01116/data/VOC2012
    drop_last: false
    img_dir: JPEGImages
    num_workers: 1
    pipeline:
    - type: LoadImageFromFile
    - flip: false
      img_scale: *id002
      transforms:
      - keep_ratio: true
        min_size: 512
        type: Resize
      - type: RandomFlip
      - mean:
        - 123.675
        - 116.28
        - 103.53
        std:
        - 58.395
        - 57.12
        - 57.375
        to_rgb: true
        type: Normalize
      - keys:
        - img
        type: ImageToTensor
      - keys:
        - img
        type: Collect
      type: MultiScaleFlipAug
    shuffle: false
    split: ImageSets/Segmentation/val.txt
    type: ZeroPascalVOCDataset20
dataset_type: ZeroPascalVOCDataset20
eval_interval: 2000
img_norm_cfg:
  mean:
  - 123.675
  - 116.28
  - 103.53
  std:
  - 58.395
  - 57.12
  - 57.375
  to_rgb: true
img_size: 512
in_channels: 512
log_interval: 50
logger:
  type: RunLogger
max_iter: 40000
model:
  backbone:
    drop_path_rate: 0.1
    get_embeddings: true
    input_resolution: 512
    layers: 12
    num_tokens: 10
    out_indices:
    - 11
    output_dim: 512
    patch_size: 16
    prompt_dim: 768
    region_level_bridge_size: 16
    total_d_layer: 11
    type: CLIPVisionTransformerWithRLB
    width: 768
  base_class:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11
  - 12
  - 13
  - 14
  both_class:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11
  - 12
  - 13
  - 14
  - 15
  - 16
  - 17
  - 18
  - 19
  class_names: *id003
  decode_head:
    all_idx:
    - 0
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    - 8
    - 9
    - 10
    - 11
    - 12
    - 13
    - 14
    - 15
    - 16
    - 17
    - 18
    - 19
    channels: 512
    embed_dims: 512
    img_size: 512
    in_channels: 512
    num_classes: 15
    num_heads: 8
    num_layers: 3
    seen_idx:
    - 0
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    - 8
    - 9
    - 10
    - 11
    - 12
    - 13
    - 14
    type: ATMSingleHeadSeg
    use_proj: false
    use_stages: 1
  exclude_key: prompt
  ft_backbone: false
  load_text_embedding: /mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/configs/_base_/datasets/text_embedding/voc12_single.npy
  novel_class:
  - 15
  - 16
  - 17
  - 18
  - 19
  pretrained: /mnt/fast/nobackup/scratch4weeks/ae01116/weights/ViT-B-16-RC-CLIP.pkl
  pretrained_text: /mnt/fast/nobackup/scratch4weeks/ae01116/weights/ViT-B-16-RC-CLIP.pkl
  test_cfg:
    crop_size: !!python/tuple
    - 512
    - 512
    mode: slide
    stride: !!python/tuple
    - 426
    - 426
  text_encoder:
    context_length: 77
    embed_dim: 512
    pretrained: /mnt/fast/nobackup/scratch4weeks/ae01116/weights/ViT-B-16-RC-CLIP.pkl
    transformer_heads: 8
    transformer_layers: 12
    transformer_width: 512
    type: CLIPTextEncoder
  type: CLIPRC
name: clip_rc_zero_vit-b_512x512_40k_voc_10_16
novel_class:
- 15
- 16
- 17
- 18
- 19
optimizer:
  betas: !!python/tuple
  - 0.9
  - 0.999
  lr: 2.0e-05
  type: CustomAdamW
  weight_decay: 0.01
out_indices:
- 11
parameter_groups_generator:
  custom_keys:
    backbone:
      lr_mult: 10.0
    head:
      lr_mult: 10.0
    ln:
      decay_mult: 0.0
    norm:
      decay_mult: 0.0
    text_encoder:
      lr_mult: 0.0
  type: CustomPrameterGroupsGenerator
pretrained: /mnt/fast/nobackup/scratch4weeks/ae01116/weights/ViT-B-16-RC-CLIP.pkl
region_level_bridge_size: 16
scheduler:
  max_steps: 40000
  min_lr: 1.0e-06
  power: 0.9
  type: PolyLR
  warmup: linear
  warmup_iters: 1500
  warmup_ratio: 1.0e-06
test_pipeline:
- type: LoadImageFromFile
- flip: false
  img_scale: *id002
  transforms:
  - keep_ratio: true
    min_size: 512
    type: Resize
  - type: RandomFlip
  - mean:
    - 123.675
    - 116.28
    - 103.53
    std:
    - 58.395
    - 57.12
    - 57.375
    to_rgb: true
    type: Normalize
  - keys:
    - img
    type: ImageToTensor
  - keys:
    - img
    type: Collect
  type: MultiScaleFlipAug
train_pipeline:
- type: LoadImageFromFile
- reduce_zero_label: true
  type: LoadAnnotations
- img_scale: *id002
  ratio_range: *id004
  type: Resize
- cat_max_ratio: 0.75
  crop_size: *id001
  type: RandomCrop
- prob: 0.5
  type: RandomFlip
- type: PhotoMetricDistortion
- mean:
  - 123.675
  - 116.28
  - 103.53
  std:
  - 58.395
  - 57.12
  - 57.375
  to_rgb: true
  type: Normalize
- pad_val: 0
  seg_pad_val: 255
  size: *id001
  type: Pad
- type: DefaultFormatBundle
- keys:
  - img
  - gt_semantic_seg
  type: Collect
work_dir: work_dirs/clip_rc_zero_vit-b_512x512_40k_voc_10_16

Thu Aug  1 14:20:18 2024  model:CLIPRC(
    backbone: CLIPVisionTransformerWithRLB(
        conv1: Conv(3, 768, (16, 16), (16, 16), (0, 0), (1, 1), 1, None, None, Kw=None, fan=None, i=None, bound=None)
        ln_pre: LayerNorm((768,), 1e-05, elementwise_affine=True)
        transformer: Transformer(
            resblocks: Sequential(
                0: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(768, 768, float32[768,], None)
                    )
                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(768, 3072, float32[3072,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(3072, 768, float32[768,], None)
                    )
                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    drop_path: Identity(None, None)
                )
                1: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(768, 768, float32[768,], None)
                    )
                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(768, 3072, float32[3072,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(3072, 768, float32[768,], None)
                    )
                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    drop_path: DropPath(p=0.00909090880304575)
                )
                2: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(768, 768, float32[768,], None)
                    )
                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(768, 3072, float32[3072,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(3072, 768, float32[768,], None)
                    )
                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    drop_path: DropPath(p=0.0181818176060915)
                )
                3: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(768, 768, float32[768,], None)
                    )
                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(768, 3072, float32[3072,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(3072, 768, float32[768,], None)
                    )
                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    drop_path: DropPath(p=0.027272727340459824)
                )
                4: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(768, 768, float32[768,], None)
                    )
                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(768, 3072, float32[3072,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(3072, 768, float32[768,], None)
                    )
                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    drop_path: DropPath(p=0.036363635212183)
                )
                5: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(768, 768, float32[768,], None)
                    )
                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(768, 3072, float32[3072,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(3072, 768, float32[768,], None)
                    )
                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    drop_path: DropPath(p=0.045454543083906174)
                )
                6: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(768, 768, float32[768,], None)
                    )
                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(768, 3072, float32[3072,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(3072, 768, float32[768,], None)
                    )
                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    drop_path: DropPath(p=0.05454545468091965)
                )
                7: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(768, 768, float32[768,], None)
                    )
                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(768, 3072, float32[3072,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(3072, 768, float32[768,], None)
                    )
                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    drop_path: DropPath(p=0.06363636255264282)
                )
                8: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(768, 768, float32[768,], None)
                    )
                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(768, 3072, float32[3072,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(3072, 768, float32[768,], None)
                    )
                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    drop_path: DropPath(p=0.072727270424366)
                )
                9: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(768, 768, float32[768,], None)
                    )
                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(768, 3072, float32[3072,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(3072, 768, float32[768,], None)
                    )
                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    drop_path: DropPath(p=0.08181817829608917)
                )
                10: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(768, 768, float32[768,], None)
                    )
                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(768, 3072, float32[3072,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(3072, 768, float32[768,], None)
                    )
                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    drop_path: DropPath(p=0.09090908616781235)
                )
                11: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(768, 768, float32[768,], None)
                    )
                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(768, 3072, float32[3072,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(3072, 768, float32[768,], None)
                    )
                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                    drop_path: DropPath(p=0.09999999403953552)
                )
            )
        )
        ln_post: LayerNorm((768,), 1e-05, elementwise_affine=True)
        prompt_proj: Linear(768, 768, float32[768,], None)
        prompt_norm: LayerNorm((768,), 1e-06, elementwise_affine=True)
        prompt_dropout: Dropout(0.1, is_train=False)
    )
    decode_head: ATMSingleHeadSeg(
        loss_decode: CrossEntropyLoss(avg_non_ignore=False)
        dropout: Dropout(0.1, is_train=False)
        input_proj_1: Identity(None, None)
        proj_norm_1: Identity(None, None)
        decoder_q: TPN_Decoder(
            layers: Sequential(
                0: TPN_DecoderLayer(
                    linear1: Linear(512, 2048, float32[2048,], None)
                    dropout: Dropout(0.1, is_train=False)
                    linear2: Linear(2048, 512, float32[512,], None)
                    norm2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    norm3: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    dropout1: Dropout(0.1, is_train=False)
                    dropout2: Dropout(0.1, is_train=False)
                    dropout3: Dropout(0.1, is_train=False)
                    multihead_attn: Attention(
                        q: Linear(512, 512, float32[512,], None)
                        k: Linear(512, 512, float32[512,], None)
                        v: Linear(512, 512, float32[512,], None)
                        attn_drop: Dropout(0.1, is_train=False)
                        proj: Linear(512, 512, float32[512,], None)
                        proj_drop: Dropout(0.0, is_train=False)
                    )
                )
                1: TPN_DecoderLayer(
                    linear1: Linear(512, 2048, float32[2048,], None)
                    dropout: Dropout(0.1, is_train=False)
                    linear2: Linear(2048, 512, float32[512,], None)
                    norm2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    norm3: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    dropout1: Dropout(0.1, is_train=False)
                    dropout2: Dropout(0.1, is_train=False)
                    dropout3: Dropout(0.1, is_train=False)
                    multihead_attn: Attention(
                        q: Linear(512, 512, float32[512,], None)
                        k: Linear(512, 512, float32[512,], None)
                        v: Linear(512, 512, float32[512,], None)
                        attn_drop: Dropout(0.1, is_train=False)
                        proj: Linear(512, 512, float32[512,], None)
                        proj_drop: Dropout(0.0, is_train=False)
                    )
                )
                2: TPN_DecoderLayer(
                    linear1: Linear(512, 2048, float32[2048,], None)
                    dropout: Dropout(0.1, is_train=False)
                    linear2: Linear(2048, 512, float32[512,], None)
                    norm2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    norm3: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    dropout1: Dropout(0.1, is_train=False)
                    dropout2: Dropout(0.1, is_train=False)
                    dropout3: Dropout(0.1, is_train=False)
                    multihead_attn: Attention(
                        q: Linear(512, 512, float32[512,], None)
                        k: Linear(512, 512, float32[512,], None)
                        v: Linear(512, 512, float32[512,], None)
                        attn_drop: Dropout(0.1, is_train=False)
                        proj: Linear(512, 512, float32[512,], None)
                        proj_drop: Dropout(0.0, is_train=False)
                    )
                )
            )
        )
        decoder_v: TPN_Decoder(
            layers: Sequential(
                0: TPN_DecoderLayer(
                    linear1: Linear(512, 2048, float32[2048,], None)
                    dropout: Dropout(0.1, is_train=False)
                    linear2: Linear(2048, 512, float32[512,], None)
                    norm2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    norm3: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    dropout1: Dropout(0.1, is_train=False)
                    dropout2: Dropout(0.1, is_train=False)
                    dropout3: Dropout(0.1, is_train=False)
                    multihead_attn: Attention(
                        q: Linear(512, 512, float32[512,], None)
                        k: Linear(512, 512, float32[512,], None)
                        v: Linear(512, 512, float32[512,], None)
                        attn_drop: Dropout(0.1, is_train=False)
                        proj: Linear(512, 512, float32[512,], None)
                        proj_drop: Dropout(0.0, is_train=False)
                    )
                )
                1: TPN_DecoderLayer(
                    linear1: Linear(512, 2048, float32[2048,], None)
                    dropout: Dropout(0.1, is_train=False)
                    linear2: Linear(2048, 512, float32[512,], None)
                    norm2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    norm3: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    dropout1: Dropout(0.1, is_train=False)
                    dropout2: Dropout(0.1, is_train=False)
                    dropout3: Dropout(0.1, is_train=False)
                    multihead_attn: Attention(
                        q: Linear(512, 512, float32[512,], None)
                        k: Linear(512, 512, float32[512,], None)
                        v: Linear(512, 512, float32[512,], None)
                        attn_drop: Dropout(0.1, is_train=False)
                        proj: Linear(512, 512, float32[512,], None)
                        proj_drop: Dropout(0.0, is_train=False)
                    )
                )
                2: TPN_DecoderLayer(
                    linear1: Linear(512, 2048, float32[2048,], None)
                    dropout: Dropout(0.1, is_train=False)
                    linear2: Linear(2048, 512, float32[512,], None)
                    norm2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    norm3: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    dropout1: Dropout(0.1, is_train=False)
                    dropout2: Dropout(0.1, is_train=False)
                    dropout3: Dropout(0.1, is_train=False)
                    multihead_attn: Attention(
                        q: Linear(512, 512, float32[512,], None)
                        k: Linear(512, 512, float32[512,], None)
                        v: Linear(512, 512, float32[512,], None)
                        attn_drop: Dropout(0.1, is_train=False)
                        proj: Linear(512, 512, float32[512,], None)
                        proj_drop: Dropout(0.0, is_train=False)
                    )
                )
            )
        )
        recovery_decoder: RecoveryDecoder(
            decouple_q: Sequential(
                0: TPN_DecoderLayer(
                    linear1: Linear(512, 2048, float32[2048,], None)
                    dropout: Dropout(0.1, is_train=False)
                    linear2: Linear(2048, 512, float32[512,], None)
                    norm2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    norm3: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    dropout1: Dropout(0.1, is_train=False)
                    dropout2: Dropout(0.1, is_train=False)
                    dropout3: Dropout(0.1, is_train=False)
                    multihead_attn: Attention(
                        q: Linear(512, 512, float32[512,], None)
                        k: Linear(512, 512, float32[512,], None)
                        v: Linear(512, 512, float32[512,], None)
                        attn_drop: Dropout(0.1, is_train=False)
                        proj: Linear(512, 512, float32[512,], None)
                        proj_drop: Dropout(0.0, is_train=False)
                    )
                )
            )
            decouple_v: Sequential(
                0: TPN_DecoderLayer(
                    linear1: Linear(512, 2048, float32[2048,], None)
                    dropout: Dropout(0.1, is_train=False)
                    linear2: Linear(2048, 512, float32[512,], None)
                    norm2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    norm3: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    dropout1: Dropout(0.1, is_train=False)
                    dropout2: Dropout(0.1, is_train=False)
                    dropout3: Dropout(0.1, is_train=False)
                    multihead_attn: Attention(
                        q: Linear(512, 512, float32[512,], None)
                        k: Linear(512, 512, float32[512,], None)
                        v: Linear(512, 512, float32[512,], None)
                        attn_drop: Dropout(0.1, is_train=False)
                        proj: Linear(512, 512, float32[512,], None)
                        proj_drop: Dropout(0.0, is_train=False)
                    )
                )
            )
            linear_q_in: Linear(512, 512, float32[512,], None)
            linear_k_in: Linear(512, 512, float32[512,], None)
            linear_q_out: Linear(512, 1024, float32[1024,], None)
            linear_k_out: Linear(512, 512, float32[512,], None)
        )
        lateral_proj: Linear(1536, 512, float32[512,], None)
        q_proj: Linear(1024, 512, float32[512,], None)
    )
    text_encoder: CLIPTextEncoder(
        transformer: Transformer(
            resblocks: Sequential(
                0: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(512, 512, float32[512,], None)
                    )
                    ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(512, 2048, float32[2048,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(2048, 512, float32[512,], None)
                    )
                    ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    drop_path: Identity(None, None)
                )
                1: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(512, 512, float32[512,], None)
                    )
                    ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(512, 2048, float32[2048,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(2048, 512, float32[512,], None)
                    )
                    ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    drop_path: Identity(None, None)
                )
                2: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(512, 512, float32[512,], None)
                    )
                    ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(512, 2048, float32[2048,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(2048, 512, float32[512,], None)
                    )
                    ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    drop_path: Identity(None, None)
                )
                3: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(512, 512, float32[512,], None)
                    )
                    ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(512, 2048, float32[2048,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(2048, 512, float32[512,], None)
                    )
                    ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    drop_path: Identity(None, None)
                )
                4: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(512, 512, float32[512,], None)
                    )
                    ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(512, 2048, float32[2048,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(2048, 512, float32[512,], None)
                    )
                    ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    drop_path: Identity(None, None)
                )
                5: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(512, 512, float32[512,], None)
                    )
                    ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(512, 2048, float32[2048,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(2048, 512, float32[512,], None)
                    )
                    ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    drop_path: Identity(None, None)
                )
                6: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(512, 512, float32[512,], None)
                    )
                    ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(512, 2048, float32[2048,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(2048, 512, float32[512,], None)
                    )
                    ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    drop_path: Identity(None, None)
                )
                7: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(512, 512, float32[512,], None)
                    )
                    ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(512, 2048, float32[2048,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(2048, 512, float32[512,], None)
                    )
                    ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    drop_path: Identity(None, None)
                )
                8: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(512, 512, float32[512,], None)
                    )
                    ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(512, 2048, float32[2048,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(2048, 512, float32[512,], None)
                    )
                    ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    drop_path: Identity(None, None)
                )
                9: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(512, 512, float32[512,], None)
                    )
                    ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(512, 2048, float32[2048,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(2048, 512, float32[512,], None)
                    )
                    ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    drop_path: Identity(None, None)
                )
                10: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(512, 512, float32[512,], None)
                    )
                    ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(512, 2048, float32[2048,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(2048, 512, float32[512,], None)
                    )
                    ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    drop_path: Identity(None, None)
                )
                11: ResidualAttentionBlock(
                    attn: MultiheadAttention(
                        out_proj: Linear(512, 512, float32[512,], None)
                    )
                    ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    mlp: Sequential(
                        c_fc: Linear(512, 2048, float32[2048,], None)
                        gelu: QuickGELU(None, None)
                        c_proj: Linear(2048, 512, float32[512,], None)
                    )
                    ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)
                    drop_path: Identity(None, None)
                )
            )
        )
        token_embedding: Embedding(49408, 512, None, dtype=None)
        ln_final: LayerNorm((512,), 1e-05, elementwise_affine=True)
    )
)
Loaded 10582 images
Loaded 1449 images
Thu Aug  1 14:20:18 2024 Start running
Traceback (most recent call last):
  File "/mnt/fast/nobackup/scratch4weeks/ae01116/zegenv/lib/python3.9/site-packages/jittor/dataset/dataset.py", line 258, in _worker_main
    batch.append(self[i])
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/jseg/datasets/custom.py", line 129, in __getitem__
    return self.prepare_train_img(idx)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/jseg/datasets/custom.py", line 136, in prepare_train_img
    return self.pipeline(results)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/jseg/datasets/pipelines/compose.py", line 22, in __call__
    data = t(data)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/jseg/datasets/pipelines/formating.py", line 23, in __call__
    results['gt_semantic_seg'] = to_tensor(
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/jseg/datasets/pipelines/formating.py", line 8, in to_tensor
    return jt.Var(data)
RuntimeError: Wrong inputs arguments, Please refer to examples(help(jt.__init__)).

Types of your inputs are:
 self	= Var,
 args	= (ndarray, ),

The function declarations are:
 VarHolder(PyObject* v, NanoString dtype=ns_void)

Failed reason:[f 0801 14:20:22.914550 28 helper_cuda.h:128] CUDA error at /mnt/fast/nobackup/scratch4weeks/ae01116/zegenv/lib/python3.9/site-packages/jittor/src/mem/allocator/cuda_host_allocator.cc:22  code=3( cudaErrorInitializationError ) cudaMallocHost(&ptr, size)

-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[54388,1],0]
  Exit code:    1
--------------------------------------------------------------------------
Beginning training script
