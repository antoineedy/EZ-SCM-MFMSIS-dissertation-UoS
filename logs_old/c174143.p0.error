2024-07-03 10:26:07,536 - mmseg - INFO - Loaded 1449 images
/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/zegenv/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
Traceback (most recent call last):
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/test.py", line 330, in <module>
    main()
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/test.py", line 291, in main
    results = single_gpu_test(
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/mmseg/apis/test.py", line 91, in single_gpu_test
    result = model(return_loss=False, **data)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/zegenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/zegenv/lib/python3.9/site-packages/mmcv/parallel/data_parallel.py", line 50, in forward
    return super().forward(*inputs, **kwargs)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/zegenv/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 166, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/zegenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/zegenv/lib/python3.9/site-packages/mmcv/runner/fp16_utils.py", line 109, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/mmseg/models/segmentors/base.py", line 110, in forward
    return self.forward_test(img, img_metas, **kwargs)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/mmseg/models/segmentors/base.py", line 92, in forward_test
    return self.simple_test(imgs[0], img_metas[0], **kwargs)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/mmseg/models/segmentors/encoder_decoder.py", line 256, in simple_test
    seg_logit = self.inference(img, img_meta, rescale)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/mmseg/models/segmentors/encoder_decoder.py", line 239, in inference
    seg_logit = self.slide_inference(img, img_meta, rescale)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/models/segmentor/inner_zegclip.py", line 337, in slide_inference
    crop_seg_logit = self.encode_decode(crop_img, img_meta)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/models/segmentor/inner_zegclip.py", line 296, in encode_decode
    out = self._decode_head_forward_test(feat, img_metas, self.self_training)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/models/segmentor/inner_zegclip.py", line 308, in _decode_head_forward_test
    seg_logits = self.decode_head.forward_test(
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/models/decode_heads/decode_seg.py", line 294, in forward_test
    return self.forward(inputs, self_training)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/models/decode_heads/decode_seg.py", line 342, in forward
    q_, attn_ = decoder_(q, lateral.transpose(0, 1))
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/zegenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/models/decode_heads/decode_seg.py", line 62, in forward
    output, attn = mod(
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/zegenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/models/decode_heads/decode_seg.py", line 96, in forward
    tgt2, attn2 = self.multihead_attn(
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/zegenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/models/decode_heads/decode_seg.py", line 142, in forward
    self.k(xk)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/zegenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/zegenv/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/zegenv/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1024x768 and 512x512)
