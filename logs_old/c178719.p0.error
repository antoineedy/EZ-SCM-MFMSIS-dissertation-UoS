[i 0801 14:05:13.797461 32 __init__.py:227] Total mem: 503.74GB, using 16 procs for compiling.
[i 0801 14:05:32.119896 32 jit_compiler.cc:28] Load cc_path: /usr/bin/g++
[i 0801 14:05:32.308844 32 init.cc:62] Found cuda archs: [86,]
[w 0801 14:05:32.434914 32 compiler.py:1384] CUDA arch(86)>80 will be backward-compatible
[i 0801 14:05:32.448848 32 __init__.py:411] Found mpicc(2.1.1) at /usr/bin/mpicc.
[i 0801 14:05:32.482613 32 compiler.py:34] Create cache dir: /user/HS400/ae01116/.cache/jittor/jt1.3.8/g++7.5.0/py3.9.19/Linux-4.18.0-4x1a/AMDEPYC753232-x73/default/cu11.0.221_sm_86/custom_ops
[i 0801 14:05:36.401530 32 compile_extern.py:339] Downloading cutt...
[i 0801 14:05:36.590712 32 compile_extern.py:352] installing cutt...
[i 0801 14:05:48.446010 32 compiler.py:34] Create cache dir: /user/HS400/ae01116/.cache/jittor/jt1.3.8/g++7.5.0/py3.9.19/Linux-4.18.0-4x1a/AMDEPYC753232-x73/default/cu11.0.221_sm_86/cuda
[i 0801 14:06:08.839504 32 cuda_flags.cc:49] CUDA enabled.

Compiling Operators(6/16) used: 3.51s eta: 5.86s Compiling Operators(13/16) used: 4.51s eta: 1.04s Compiling Operators(16/16) used: 5.51s eta:    0s 

Compiling Operators(3/4) used: 3.32s eta: 1.11s Compiling Operators(4/4) used: 4.32s eta:    0s 

Compiling Operators(1/2) used: 3.33s eta: 3.33s Compiling Operators(2/2) used: 4.33s eta:    0s 
/mnt/fast/nobackup/users/ae01116/multi-modal-dissertation-uos/jseg/models/losses/cross_entropy_loss.py:55: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with Jittor official cross_entropy, set ``avg_non_ignore=True``.
  warnings.warn(

Compiling Operators(6/6) used: 4.36s eta:    0s 

Compiling Operators(4/5) used: 3.39s eta: 0.847s Compiling Operators(5/5) used: 4.39s eta:    0s 

Compiling Operators(3/3) used: 4.36s eta:    0s 

Compiling Operators(3/3) used: 4.34s eta:    0s 

Compiling Operators(4/4) used: 4.37s eta:    0s 
[e 0801 14:06:49.881871 32 log.cc:258] Caught SIGINT, quick exit
