####################
#
# Example Job for HTCondor
#
####################

#---------------------------------------------
# Name your batch so it's easy to distinguish in the q.
JobBatchName = "Antoine-ZegCLIP-1"

# --------------------------------------------
# Executable and its arguments
executable    = /user/HS400/ae01116/miniconda3/envs/dissert/bin/python3
arguments     = $ENV(PWD)/start_training.py

# ---------------------------------------------------
# Universe (vanilla, docker)
universe         = docker
docker_image     = pytorch/pytorch

# -------------------------------------------------
# Event, out and error logs
log    = logs/c$(cluster).p$(process).log
output = logs/c$(cluster).p$(process).out
error  = logs/c$(cluster).p$(process).error

# -----------------------------------
# File Transfer, Input, Output
should_transfer_files = YES
when_to_transfer_output = ON_EXIT

# transfer all the files in the current directory

# transfer_input_files = $ENV(PWD)
#transfer_input_files = $ENV(PWD)/antoine-zeg.submit_file, $ENV(PWD)/train.py, $ENV(PWD)/get_flops.py, $ENV(PWD)/logs, $ENV(PWD)/models, $ENV(PWD)/data, $ENV(PWD)/docker_stderror, $ENV(PWD)/start_training.py, $ENV(PWD)/test.py, $ENV(PWD)/dist_train.sh, $ENV(PWD)/weights, $ENV(PWD)/configs
transfer_input_files = $ENV(PWD)/start_training.py, $ENV(PWD)/dist_train.sh

# Make certain project spaces available in container
# Uncomment this environment line if you're not running on /mnt/fast
environment = "mount=$ENV(PWD)"

# -------------------------------------
# Requirements for the Job (Requirements are explained in further detail in example09.submit_file)
# NOTE: HasStornext is not valid on orca.
requirements = (CUDAGlobalMemoryMb > 4500) && (CUDAGlobalMemoryMb <  17000) && \
#              (HasStornext) && \
			   (CUDACapability > 2.0)

# --------------------------------------
# Resources
request_GPUs     = 1
# this needs to be specified for the AI@Surrey cluster if requesting a GPU
+GPUMem          = 10000  
request_CPUs   = 3 
# tailor the number of CPUs to your needs - check epoch time or GPU power usage via 
# https://wandb.ai/site to know that you're not staving the GPU
request_memory   = 2G

#This job will complete in less than 1 hour
+JobRunTime = 1

#This job can checkpoint
+CanCheckpoint = true

# -----------------------------------
# Queue commands
queue
